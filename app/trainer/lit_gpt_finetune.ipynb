{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-6UGctNwnA_"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-gpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements-all.txt"
      ],
      "metadata": {
        "id": "AwTrFU7YyuBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!python scripts/download.py --repo_id microsoft/phi-2 --from_safetensors True\n",
        "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/microsoft/phi-2"
      ],
      "metadata": {
        "id": "DaoBDYoA2mTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/prepare_csv.py --csv_path /content/drive/MyDrive/dataset/MedQA-Dataset.csv --checkpoint_dir /content/checkpoints/microsoft/phi-2"
      ],
      "metadata": {
        "id": "30aplT111heH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune/adapter.py --checkpoint_dir /content/checkpoints/microsoft/phi-2 --data_dir /content/data/csv --out_dir /content/out/adapter/phi-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFygwZ55Er3c",
        "outputId": "ab88bdda-821e-4e95-d2ba-9574f82d49fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 16-bit Automatic Mixed Precision (AMP)\n",
            "{'eval_interval': 600, 'save_interval': 1000, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.003, 'batch_size': 64.0, 'micro_batch_size': 1, 'gradient_accumulation_iters': 64.0, 'epoch_size': 50000, 'num_epochs': 5, 'max_iters': 250000, 'weight_decay': 0.02, 'warmup_steps': 1562.0}\n",
            "Seed set to 1337\n",
            "Loading model '/content/checkpoints/microsoft/phi-2/lit_model.pth' with {'name': 'phi-2', 'hf_config': {'org': 'microsoft', 'name': 'phi-2'}, 'block_size': 2048, 'vocab_size': 50257, 'padding_multiple': 512, 'padded_vocab_size': 51200, 'n_layer': 32, 'n_head': 32, 'n_embd': 2560, 'rotary_percentage': 0.4, 'parallel_residual': True, 'bias': True, 'lm_head_bias': True, 'n_query_groups': 32, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'tanh', 'intermediate_size': 10240, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'adapter_prompt_length': 10, 'adapter_start_layer': 2, 'head_size': 80, 'rope_n_elem': 32}\n",
            "Number of trainable parameters: 768,960\n",
            "Number of non trainable parameters: 2,779,683,840\n",
            "Seed set to 1337\n",
            "The longest sequence length in the train data is 2048, the model's maximum sequence length is 2048 and context length is 2048\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "I recommend the movie \"Parasite\" for you to watch during the weekend. It is a critically acclaimed Korean film that won several awards, including Best Picture at the Academy Awards. The movie is a thrilling and thought-provoking story about social class and inequality in South Korea. I think you will enjoy it because it has a gripping plot, excellent acting, and a surprising twist at the end.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetune/adapter.py\", line 276, in <module>\n",
            "    CLI(setup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 96, in CLI\n",
            "    return _run_component(components, cfg_init)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 181, in _run_component\n",
            "    return component(**cfg)\n",
            "  File \"/content/finetune/adapter.py\", line 75, in setup\n",
            "    fabric.launch(main, data_dir, checkpoint_dir, out_dir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
            "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/content/finetune/adapter.py\", line 110, in main\n",
            "    train(fabric, model, optimizer, train_data, val_data, checkpoint_dir, out_dir)\n",
            "  File \"/content/finetune/adapter.py\", line 157, in train\n",
            "    logits = model(input_ids, lm_head_chunk_size=128)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/wrappers.py\", line 119, in forward\n",
            "    output = self._forward_module(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/lit_gpt/adapter.py\", line 67, in forward\n",
            "    x = block(x, cos, sin, mask, input_pos)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/lit_gpt/model.py\", line 154, in forward\n",
            "    h = self.attn(n_1, cos, sin, mask, input_pos)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/lit_gpt/model.py\", line 224, in forward\n",
            "    y = self.scaled_dot_product_attention(q, k, v, mask)\n",
            "  File \"/content/lit_gpt/adapter.py\", line 146, in scaled_dot_product_attention\n",
            "    return y + self.gating_factor * ay\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 259325 has 14.74 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 118.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    }
  ]
}